---
type: docs 
title: 【Lecture 2】机器学习调优
date: 2022-11-27
featured: true
draft: false
comment: true
toc: true
reward: false
pinned: false
carousel: false
math: true
series:
  - lhyml
---

在这一章中，首先大概讲了一些机器学习中常见问题的通用解决方法，告诉了我们可以怎样去处理。接着针对类神经网络训练不起来的问题，从局部最小值、批次与动量、学习率和损失函数四个点上分别进行了讲解。

<!--more-->

## 一、机器学习任务攻略

<img src="https://img.yulegend.cn/img/image-20221127163051527.png" style="zoom:33%;" />

### 训练集上的loss太大了

#### 模型偏差大(Model bias)

- 模型太简单了（need more flexible）
  - 输入更多的特征$x_i$
  - 使用更多的神经元(neuron)
  - 增加网络的深度(layer)
- 当然也是不能让模型过于的复杂，选择一个恰当的点，这是一直需要考虑的问题
  - <img src="https://img.yulegend.cn/img/image-20221127171221552.png" alt="image-20221127171221552" style="zoom:33%;" />

#### 优化问题



#### Model Bias or Optimization Issue ?

- 通过比较不同复杂度的模型的loss
  - 在简单的模型能够得到比较低的loss，但是却在复杂的模型上得到的loss更高，那这个时候就可以考虑是Optimization的问题，因为之前有说，增强模型复杂度，可以缓解Model Bias，但现在明明模型更复杂了，但是loss反而增大了。
- 因此在处理一个新问题时，可以先选择一个简单的模型，线性模型，或者传统机器学习处理，这样一般存在优化的问题。



### 训练集上的loss较小，测试集上的loss大

#### Overfitting

- 更多的训练数据（more training data）
  - 视频中有说更多的数据能够更好地限制住模型s
  - 但更准确地说应该是更多的训练数据，会符合整体数据的分布，也就是说只要我的训练数据的分布得到的模型，能够和测试数据的一样，那就能够得到比较好的结果。
- 数据增强（data augmentation）
  - 像图像，可以通过平移、翻转等操作增加出更多有意义的数据
  - 信号的话，也可以通过偏移，来增强数据
- 限制模型的复杂度
  - 简单一些的模型对应的解也会更少，所以可以更快找到比较优的结果（当然也是不能让模型过于简单，太简单的话又会回到欠拟合的情况下）
  - Less parameters(减少神经元)，sharing parameters(CNN)
    - <img src="https://img.yulegend.cn/img/image-20221127171029692.png" alt="image-20221127171029692" style="zoom:25%;" />
  - 更少的特征
  - 早停（early stopping）
  - 正则化（regularization）
  - Dropout

#### mismatch

- 有点类似overfitting，但overfittin更多说的是训练和测试数据的分布应该是差不多的
- 但是在mismatch的情况下，由于测试数据的分布和训练数据的分布差距很大，这样在测试集上的结果肯定是不准的。



### 将训练数据划分为训练集和验证集

<img src="https://img.yulegend.cn/img/image-20221127182418546.png" alt="image-20221127182418546" style="zoom: 33%;" />

这里考虑的点，让我有种醍醐灌顶的感觉。

就是为什么在比赛通常会划分为public和private的测试集？

- 以前我个人以为只是做进一步的评估，这样会更充分一些，因为测试数据越多也越有说服力，或者说private的测试集的分布会有些差异。
- 但其实我们的前提就是训练集和测试集的分布要相似的，这是需要明确的。
- 而为什么会有private测试集，是因为我们在public的测试集上能够看到得分，那只要不断地对照在public上的分数，随机生成结果提交，最终一定能得到比较高的分数，但这其实是毫无意义的。
- 所以需要将训练数据拆分出验证集，然后我们根据验证集上的最好结果，来进行提交。这个时候，虽然能够看得到在public测试集上的得分，但最好也不要去在意，因为如果这个时候又去根据public上的结果去调整，那又回到了上个小点追求高的public测试集的分，但在private上得分低的情况。
- 因此，只要关注验证集就好，那为了让验证集上的结果也更可靠，可以使用K交叉验证(K-Fold cross validation)的方法。



## 二、类神经网络训练不起来怎么办？

### 1. 梯度为0，导致网络不再更新

- Local minima VS. Saddle point
  - 首先需要明确，在SGD优化下，并不只有局部最小值的问题，还有鞍点（Saddle point）的问题
  - 在局部最小值的时候，已经是这个区间内的最小值了，但在鞍点时，虽然一阶导为0，但是鞍点在这个区间内，既不是最小值，也不是最大值。

#### 数学解释

<img src="https://img.yulegend.cn/img/image-20221127211433066.png" alt="image-20221127211433066" style="zoom: 33%;" />

从上图可以知道，我们将损失函数$L(\theta)$近似为了一个多项式，其实也就是高数里的泰勒展开，这里放一下在某一点的泰勒展开的定义：

> 若一个函数在某个区间内可以求任意阶的导数（例如幂函数，三角函数，指数函数，对数函数等），那么这个函数可以用一个多项式近似。

对应于我们这里就是$\theta^{'}$是一个特定的点，而$\theta$是$\theta^{'}$附近的点（左右），这里的$g$就是一阶导，$H$就是二阶导（由于是多元微分中，所以是一个海森矩阵）。从这里就已经可以理解，为什么我们每次在讲loss的时候，总是会说到一阶动量和二阶动量，因为他们与loss的优化是密切相关的。

<img src="C:%5CUsers%5Cy2554%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20221127212308723.png" alt="image-20221127212308723" style="zoom:33%;" />

那当我们的梯度为0，陷入到了局部最小值或鞍点的时候，那$g$为0，按上图就直接去掉了，这时就只需要考虑后面的一部分。看完下面的解释，就终于可以理解以前为什么老是说一个函数的海森矩阵半正定就是一个凸函数了。

<img src="https://img.yulegend.cn/img/image-20221127212836387.png" alt="image-20221127212836387" style="zoom:33%;" />

可以看到，如果$H>0$，也就是$H$正定，那可以得到极小值，这和以前高数中学的一样（一阶导为0，二阶导大于0）。类似的，$H<0$，$H$是负正定，得到极大值（不过这里不用太关注极大值的问题，因为如果目标是最大值，那就加个负号，或倒一下，就变成求最小值了）。最后的一种情况，也就是以前最迷惑的一点，$H$既可以大于0，又可以小于0，则可以得到这个点是鞍点，在鞍点的时候，我们是还可以继续走，找到更小的loss的。

对于半正定其实也是类似的，就假设最极端地情况，$H$为0了，那这样的结果也就是周围的点和$\theta^{'}$相等了。

#### 海森矩阵解决鞍点问题

当遇到鞍点问题的时候，此时是可以根据海森矩阵的特征值来求解参数的优化方向的：

- 因为是鞍点，所以是存在小于0的特征值，根据这个特征值求得对应的特征向量
- 可得下一步移动的方向就是该特征向量的方向

<img src="https://img.yulegend.cn/img/image-20221128123029918.png" style="zoom:33%;" />

但是也有提及这个方法在实际中是很少用到的，计算量大。这个问题感觉可以类比为什么我们通常是用梯度下降的方法来优化，而不是直接去求一阶导数为0的（当然这只针对凸函数），但计算机是更适合去做重复性的事情。

#### 在训练中，鞍点or局部最小值，哪种情况比较多？

<img src="https://img.yulegend.cn/img/image-20221128123423468.png" alt="image-20221128123423468" style="zoom:33%;" />

结论是鞍点的情况是远多于局部最小值的情况的，即使是遇到了局部最小值的情况，在更高的维度下也是会转变为鞍点问题。

### 2. Batch & Momentum

#### Small Batch or Large Batch?

- 大Batch的速度其实比小Batch要快（因为有GPU并行运算）
  - 跑一个大Batch和小Batch的时间差不多，但是小Batch需要循环迭代更多次数
- 小Batch的效果往往会比大Batch的效果要好
  - 给出的解释是小的Batch更加Noisy，每一次会得到的loss差异大，而大Batch下求的平均loss会平滑掉，没那么容易冲出局部最小值。
  - <img src="https://img.yulegend.cn/img/image-20221128125720020.png" alt="image-20221128125720020" style="zoom:33%;" />
- 小Batch的泛化能力会更好（大小Batch在训练集上达到相同的准确率，但是小batch在测试集上的表现更好）
  - 小的batch更容易冲出狭窄的loss最小值，停留在比较宽广的最小值
  - 测试集和训练集的分布会有些差异，若停留在训练集狭窄的最低点，但在测试集上的差异可能就会比较大
  - <img src="https://img.yulegend.cn/img/image-20221128130159679.png" alt="image-20221128130159679" style="zoom:33%;" />